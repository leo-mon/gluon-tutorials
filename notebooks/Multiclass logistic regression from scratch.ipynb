{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形回帰のパートを終えたなら一番難しい部分は終わった（そりゃそーだ\n",
    "- どうデータを操作する不法\n",
    "- 計算グラフをその場で生成する方法、導関数を出す方法\n",
    "- 損失関数、モデルのたてかた、オプティマイザの書き方\n",
    "\n",
    "もう後は大体同じ、違うのはデータの量とモデルの複雑さ  \n",
    "1,2年で新しいオプティマイザ（ hipterってなんやねんhipterって）が出てくるが、大元はSGD\n",
    "\n",
    "次にやるのは数字の分類、多クラスロジスティック回帰…ソフトマックス回帰や多項回帰として知られるもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()  # コンテクスト、GPUがあるならそっちでもいいよとのこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MNIST dataset\n",
    "28*28の白黒手書き画像を10種に分類  \n",
    "最初にMXNetのユーティリティでデータ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = mx.test_utils.get_mnist()  #　バイナリでとってくる？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4つのキーからなるオブジェクトがこれで手に入る: `train_data`, `train_label`, `test_data`, `test_label`  > train-rabelになってる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mnist['train_data'][0]\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各画像は3つのタプルからなっている（channel, height, width）  \n",
    "これがカラーだと、channelが3つの要素になる(RGB)　　\n",
    "\n",
    "ラベルのチェックもできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = mnist['train_label'][0]\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習のライブラリは一般的に画像を(batch, channel, height, width)のフォーマットで扱う  \n",
    "一方で大半の画像処理のライブラリは(height, width channel)の順  \n",
    "こちらの形に変換する　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im = np.tile(image.transpose(1,2,0),(1,1,3))  \n",
    "# transpose(1,2,0): この場合配列要素を1,2,0の順に（今回channel, height, width=>height, width, channel）\n",
    "# np.tile(array, (1,1,3)): arrayをもとに1回、１回、３回リピートして生成\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[0,1,2],[3,4,5],[6,7,8]],[[9,10,11],[12,13,14],[15,16,17]]])\n",
    "print(a)\n",
    "print(a.transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この例だと14は(1,2,3)。これをtranspose(1,2,0)すると(2,3,1)へと来る\n",
    "\n",
    "取得したMNISTのデータは\n",
    "- 一番大きな枠組みがチャネル [R,G,B]\n",
    "- その中にheight: Rの中を見ると[[...],[...],[...]]\n",
    "- その中にwidth: [...]\n",
    "\n",
    "一方画像処理は\n",
    "- 一番大きな枠組みがheight [[...],[...],[...]]\n",
    "- その次にwidth: [...]\n",
    "- その中にchannel: 一つの . が[R,G,B]\n",
    "(文字だとわかりづらいな……)\n",
    "\n",
    "tile()で3回リピートすることでRGBを満たすようにしている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "描画すると"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(im)\n",
    "plt.show() # ここもチュートリアルリンク途切れてる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data iterator\n",
    "イテレータにロードする、これで楽ができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data = mx.io.NDArrayIter(mnist['train_data'], mnist['train_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニングした後にテストデータへのモデル適用を行う、さもないとみんな知ってる通り、モデルは馬鹿げたことをしだす：トレーニングのデータを覚えてただ単にラベルを吐き出すだけ（過学習のことを言いたいのかな"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = mx.io.NDArrayIter(mnist[\"test_data\"], mnist[\"test_label\"], batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allocate model parameters\n",
    "モデルの定義、今回はマルチモーダルな部分は捨てて単に1Dベクトル: 28*28=784 のコンポーネントとして見る  \n",
    "多クラス分類なので入力Xに対して確率を求める必要がある  \n",
    "$$\n",
    "P(Y=c|X)\n",
    "$$\n",
    "784の入力に対し、出力は10クラスなので784*10の行列へ落とす  \n",
    "オフセット（バイアスb）は要素10のベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = nd.random_normal(shape=(784,10))\n",
    "b = nd.random_normal(shape=10)\n",
    "\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでのように、MXNetにトレーニングの時に自動で勾配を求められるよう宣言しておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass logistic regression\n",
    "今回はクラス分類、Xを一つのLへと結びつけたい  \n",
    "基本的な考え方はXを10個の異なる実数値(`y_linear`)へとマッピングする  \n",
    "これをする前に、アウトプットを正の値かつ合計値を１へとノーマライズする  \n",
    "この正規化によりyhatを明確な確率分布へとすることができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear - nd.max(y_linear))\n",
    "    partition = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト\n",
    "sample_y_linear = nd.random_normal(shape=(2,10))\n",
    "sample_yhat = softmax(sample_y_linear)\n",
    "print(sample_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合計値が1になるかの確認\n",
    "print(nd.sum(sample_yhat, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    y_linear = nd.dot(X, W) + b\n",
    "    yhat = softmax(y_linear)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cross-entropy loss function\n",
    "確率分布による予測でも意味をなす損失関数を定義する必要がある  \n",
    "最も有名なものはクロスエントロピー関数、回帰よりもあいまいなため  \n",
    "基本的な考え方はターゲットYをOne hot ベクトルとする: [0,1,0,0,0,0,0,0,0,0]　　\n",
    "これに対しどのくらい予測がマッチしてるかを数値化する  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    return - nd.sum(y * nd.log(yhat), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "同様にSGDを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write evaluation loop to calculate accuracy\n",
    "クロスエントロピーは素晴らしい一方で、人間の評価の方法とは違う  \n",
    "正解/総数で評価するのが直感的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    \n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        with autograd.record():\n",
    "            data = batch.data[0].as_in_context(ctx).reshape((-1, 784))\n",
    "            label = batch.label[0].as_in_context(ctx)\n",
    "            label_one_hot = nd.one_hot(label, 10)\n",
    "            output = net(data)\n",
    "        \n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "        # print(numerator)\n",
    "        # print(denominator)\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この下の計算はモデルをランダムに初期化しているので、また大雑把に1/10がそれぞれのクラスに属しているので、.10程度の正確さになるはず"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_accracy(test_data, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "moving_loss = 0.\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        with autograd.record():\n",
    "            data = batch.data[0].as_in_context(ctx).reshape((-1, 784))\n",
    "            label = batch.label[0].as_in_context(ctx)\n",
    "            label_one_hot = nd.one_hot(label, 10)\n",
    "            output = net(data)\n",
    "            loss = cross_entropy(output, label_one_hot)\n",
    "            loss.backward()\n",
    "        SGD(params, .001)\n",
    "        \n",
    "        \n",
    "        if i == 0:\n",
    "            moving_loss = np.mean(loss.asnumpy()[0])\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * np.mean(loss.asnumpy()[0])\n",
    "    \n",
    "    # if i % 100 == 0:\n",
    "    test_accuracy = evaluate_accracy(test_data, net)\n",
    "    train_accuracy = evaluate_accracy(train_data, net)\n",
    "    print('Epoch {}. Loss: {}, Train_acc {}, Test_acc {}'.format(e, moving_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> やはり手元PCだと時間かかる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
