{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前節では`mx.ndarray`と`mxnet.autograd`を利用してスクラッチで組んだがもっと簡単に実装する方法あり。  \n",
    "今回は`gluon`を利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はコンテクストはCPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the dataset\n",
    "前回と同様にデータセットを準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.randn(10000,2)\n",
    "Y = 2* X[:,0] - 3.4 * X[:,1] + 4.2 + .01 * np.random.normal(size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data iterator\n",
    "バッチ処理のためのイテレータの導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_data = mx.io.NDArrayIter(X, Y, batch_size, shuffle=True)  # なぜに今回はyも大文字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "前回は各パラメータを配置した後、モデルとしてまとめた。  \n",
    "スクラッチでどんなものか確認する際にはいいことだが、`gluon`を使えば、あらかじめ定義された層からネットワークをまとめることができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "net.add(gluon.nn.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize parameters\n",
    "モデルに対して何か操作を施す前に、重みを初期化する必要がある  \n",
    "MXNetは`mxnet.init`で、共通の初期化関数を提供している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difine loss\n",
    "ロスを自身で定義せず、今回は`gluon.loss.L2Loss`を利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "SGDをスクラッチで書くかわりに、`gluon.Trainer`にパラメータの辞書を渡して利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute training loop\n",
    "これでループを書く全ての手順が整った。  \n",
    "最初に`epochs`の定義をする  \n",
    "次にそれぞれの試行で、`train_data` から値とそのラベルのバッチを取得、イテレートしていく\n",
    "\n",
    "それぞれのバッチの中では以下のような過程を経る  \n",
    "\n",
    "- `yhat`予測と`loss`損失の生成: 順伝播\n",
    "- 勾配計算:逆伝播(`loss.backward()`)\n",
    "- SGDによるモデルのパラメータのアップデート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 0. Moving avg of loss: 17.40207290649414\n",
      "Epoch 0, batch 500. Moving avg of loss: 0.11727231408217394\n",
      "Epoch 0, batch 1000. Moving avg of loss: 0.0008214371748440522\n",
      "Epoch 0, batch 1500. Moving avg of loss: 5.698019568234263e-05\n",
      "Epoch 0, batch 2000. Moving avg of loss: 4.182771279906423e-05\n",
      "Epoch 1, batch 0. Moving avg of loss: 0.00027339678490534425\n",
      "Epoch 1, batch 500. Moving avg of loss: 5.591434771157916e-05\n",
      "Epoch 1, batch 1000. Moving avg of loss: 5.126880809587632e-05\n",
      "Epoch 1, batch 1500. Moving avg of loss: 5.1919817488819594e-05\n",
      "Epoch 1, batch 2000. Moving avg of loss: 4.179446366995547e-05\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "ctx = mx.cpu()\n",
    "moving_loss = 0.\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx).reshape((-1,1))\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            mse = loss(output, label)\n",
    "        mse.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        ##########################\n",
    "        # ロスの移動平均を保持\n",
    "        ##########################\n",
    "        if i == 0:\n",
    "            moving_loss = np.mean(mse.asnumpy()[0])\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * np.mean(mse.asnumpy()[0])\n",
    "            \n",
    "        if i % 500 == 0:\n",
    "            print('Epoch {}, batch {}. Moving avg of loss: {}'.format(e, i, moving_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Wとbを取り出す方法がわからない、進めてけばわかる？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
